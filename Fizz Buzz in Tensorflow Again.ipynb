{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although I failed in the interview last time, I still consider that neural networks have ability to learn and solve \"fizz buzz\". \n",
    "So I came back to modify the structure of my neural network.\n",
    "\n",
    "即便經過上次面試的教訓，我仍然相信神經網路是有能力學會「Fizz Buzz」問題的，於是回來調整網路的架構。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: return np.array([0, 0, 0, 1])\n",
    "    elif i % 5  == 0: return np.array([0, 0, 1, 0])\n",
    "    elif i % 3  == 0: return np.array([0, 1, 0, 0])\n",
    "    else            : return np.array([1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DIGITS = 10\n",
    "trX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, 2 ** NUM_DIGITS)])\n",
    "trY = np.array([fizz_buzz_encode(i)          for i in range(101, 2 ** NUM_DIGITS)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's increase the number of hidden units to 1000.\n",
    "\n",
    "這次，我增加隱藏層單元至1000個（10倍）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HIDDEN = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, NUM_DIGITS])\n",
    "Y = tf.placeholder(\"float\", [None, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "w_h = init_weights([NUM_DIGITS, NUM_HIDDEN])\n",
    "w_o = init_weights([NUM_HIDDEN, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, w_h, w_o):\n",
    "    h = tf.nn.relu(tf.matmul(X, w_h))\n",
    "    return tf.matmul(h, w_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interest of time, I raised the learning rate to 0.5 so we have larger adjustments of back propagation in each epoch. \n",
    "Note that a neural network is unable to converge with a too large learning rate.\n",
    "\n",
    "為了縮短收斂時間，調大learning rate，提高每epoch反向傳遞時的梯度修正，但過大反而會無法收斂，因此我使用0.5作為這次的learning rate（10倍）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_x = model(X, w_h, w_o)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\n",
    "train_op = tf.train.GradientDescentOptimizer(0.5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_op = tf.argmax(py_x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz(i, prediction):\n",
    "    return [str(i), \"fizz\", \"buzz\", \"fizzbuzz\"][prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could reduce it to 1000 epochs for training.\n",
    "\n",
    "並從原先10000個epoch數減少為1000次（1/10倍）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can print out the training accuracy on each epoch.\n",
    "\n",
    "同樣地，在每個階段印出epoch和在訓練資料上的準確率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.534127843987\n",
      "1 0.534127843987\n",
      "2 0.534127843987\n",
      "3 0.534127843987\n",
      "4 0.534127843987\n",
      "5 0.534127843987\n",
      "6 0.534127843987\n",
      "7 0.534127843987\n",
      "8 0.534127843987\n",
      "9 0.534127843987\n",
      "10 0.534127843987\n",
      "11 0.534127843987\n",
      "12 0.534127843987\n",
      "13 0.534127843987\n",
      "14 0.534127843987\n",
      "15 0.534127843987\n",
      "16 0.534127843987\n",
      "17 0.534127843987\n",
      "18 0.534127843987\n",
      "19 0.534127843987\n",
      "20 0.534127843987\n",
      "21 0.534127843987\n",
      "22 0.534127843987\n",
      "23 0.534127843987\n",
      "24 0.534127843987\n",
      "25 0.534127843987\n",
      "26 0.534127843987\n",
      "27 0.534127843987\n",
      "28 0.534127843987\n",
      "29 0.534127843987\n",
      "30 0.534127843987\n",
      "31 0.534127843987\n",
      "32 0.534127843987\n",
      "33 0.534127843987\n",
      "34 0.534127843987\n",
      "35 0.534127843987\n",
      "36 0.534127843987\n",
      "37 0.534127843987\n",
      "38 0.534127843987\n",
      "39 0.534127843987\n",
      "40 0.534127843987\n",
      "41 0.534127843987\n",
      "42 0.534127843987\n",
      "43 0.534127843987\n",
      "44 0.534127843987\n",
      "45 0.534127843987\n",
      "46 0.548212351029\n",
      "47 0.534127843987\n",
      "48 0.534127843987\n",
      "49 0.547128927411\n",
      "50 0.56338028169\n",
      "51 0.554712892741\n",
      "52 0.534127843987\n",
      "53 0.569880823402\n",
      "54 0.561213434453\n",
      "55 0.543878656555\n",
      "56 0.581798483207\n",
      "57 0.561213434453\n",
      "58 0.552546045504\n",
      "59 0.5872156013\n",
      "60 0.576381365114\n",
      "61 0.577464788732\n",
      "62 0.534127843987\n",
      "63 0.556879739978\n",
      "64 0.613217768147\n",
      "65 0.605633802817\n",
      "66 0.608884073673\n",
      "67 0.589382448537\n",
      "68 0.606717226436\n",
      "69 0.53954496208\n",
      "70 0.630552546046\n",
      "71 0.538461538462\n",
      "72 0.613217768147\n",
      "73 0.565547128927\n",
      "74 0.564463705309\n",
      "75 0.574214517876\n",
      "76 0.593716143012\n",
      "77 0.628385698808\n",
      "78 0.639219934995\n",
      "79 0.637053087757\n",
      "80 0.599133261105\n",
      "81 0.5872156013\n",
      "82 0.578548212351\n",
      "83 0.559046587216\n",
      "84 0.568797399783\n",
      "85 0.550379198267\n",
      "86 0.606717226436\n",
      "87 0.550379198267\n",
      "88 0.613217768147\n",
      "89 0.667388949079\n",
      "90 0.541711809317\n",
      "91 0.670639219935\n",
      "92 0.554712892741\n",
      "93 0.723726977248\n",
      "94 0.565547128927\n",
      "95 0.598049837486\n",
      "96 0.654387865655\n",
      "97 0.4127843987\n",
      "98 0.691224268689\n",
      "99 0.537378114843\n",
      "100 0.700975081257\n",
      "101 0.540628385699\n",
      "102 0.546045503792\n",
      "103 0.565547128927\n",
      "104 0.665222101842\n",
      "105 0.71397616468\n",
      "106 0.582881906826\n",
      "107 0.641386782232\n",
      "108 0.764897074756\n",
      "109 0.570964247021\n",
      "110 0.60346695558\n",
      "111 0.692307692308\n",
      "112 0.656554712893\n",
      "113 0.744312026002\n",
      "114 0.83315276273\n",
      "115 0.340195016251\n",
      "116 0.774647887324\n",
      "117 0.626218851571\n",
      "118 0.660888407367\n",
      "119 0.683640303359\n",
      "120 0.650054171181\n",
      "121 0.705308775731\n",
      "122 0.758396533044\n",
      "123 0.707475622969\n",
      "124 0.8255687974\n",
      "125 0.459371614301\n",
      "126 0.771397616468\n",
      "127 0.704225352113\n",
      "128 0.593716143012\n",
      "129 0.667388949079\n",
      "130 0.841820151679\n",
      "131 0.71397616468\n",
      "132 0.703141928494\n",
      "133 0.593716143012\n",
      "134 0.846153846154\n",
      "135 0.6511375948\n",
      "136 0.723726977248\n",
      "137 0.607800650054\n",
      "138 0.70639219935\n",
      "139 0.735644637053\n",
      "140 0.829902491874\n",
      "141 0.765980498375\n",
      "142 0.464788732394\n",
      "143 0.798483206934\n",
      "144 0.901408450704\n",
      "145 0.906825568797\n",
      "146 0.523293607801\n",
      "147 0.822318526544\n",
      "148 0.811484290358\n",
      "149 0.774647887324\n",
      "150 0.595882990249\n",
      "151 0.887323943662\n",
      "152 0.67497291441\n",
      "153 0.799566630553\n",
      "154 0.721560130011\n",
      "155 0.875406283857\n",
      "156 0.671722643554\n",
      "157 0.839653304442\n",
      "158 0.643553629469\n",
      "159 0.933911159263\n",
      "160 0.867822318527\n",
      "161 0.884073672806\n",
      "162 0.569880823402\n",
      "163 0.824485373781\n",
      "164 0.936078006501\n",
      "165 0.687973997833\n",
      "166 0.930660888407\n",
      "167 0.905742145179\n",
      "168 0.960996749729\n",
      "169 0.702058504875\n",
      "170 0.931744312026\n",
      "171 0.853737811484\n",
      "172 0.866738894908\n",
      "173 0.66630552546\n",
      "174 0.890574214518\n",
      "175 0.931744312026\n",
      "176 0.946912242687\n",
      "177 0.793066088841\n",
      "178 0.932827735645\n",
      "179 0.925243770314\n",
      "180 0.942578548212\n",
      "181 0.950162513543\n",
      "182 0.962080173348\n",
      "183 0.901408450704\n",
      "184 0.940411700975\n",
      "185 0.89707475623\n",
      "186 0.939328277356\n",
      "187 0.958829902492\n",
      "188 0.87323943662\n",
      "189 0.867822318527\n",
      "190 0.957746478873\n",
      "191 0.685807150596\n",
      "192 0.925243770314\n",
      "193 0.930660888407\n",
      "194 0.823401950163\n",
      "195 0.956663055255\n",
      "196 0.903575297941\n",
      "197 0.959913326111\n",
      "198 0.963163596966\n",
      "199 0.957746478873\n",
      "200 0.975081256771\n",
      "201 0.940411700975\n",
      "202 0.860238353196\n",
      "203 0.824485373781\n",
      "204 0.964247020585\n",
      "205 0.953412784399\n",
      "206 0.962080173348\n",
      "207 0.924160346696\n",
      "208 0.967497291441\n",
      "209 0.967497291441\n",
      "210 0.926327193933\n",
      "211 0.90465872156\n",
      "212 0.957746478873\n",
      "213 0.971830985915\n",
      "214 0.972914409534\n",
      "215 0.970747562297\n",
      "216 0.975081256771\n",
      "217 0.960996749729\n",
      "218 0.978331527627\n",
      "219 0.977248104009\n",
      "220 0.97616468039\n",
      "221 0.902491874323\n",
      "222 0.88082340195\n",
      "223 0.898158179848\n",
      "224 0.913326110509\n",
      "225 0.975081256771\n",
      "226 0.967497291441\n",
      "227 0.891657638137\n",
      "228 0.979414951246\n",
      "229 0.80931744312\n",
      "230 0.969664138678\n",
      "231 0.945828819068\n",
      "232 0.980498374865\n",
      "233 0.960996749729\n",
      "234 0.735644637053\n",
      "235 0.955579631636\n",
      "236 0.96858071506\n",
      "237 0.892741061755\n",
      "238 0.97616468039\n",
      "239 0.945828819068\n",
      "240 0.978331527627\n",
      "241 0.977248104009\n",
      "242 0.981581798483\n",
      "243 0.94474539545\n",
      "244 0.947995666306\n",
      "245 0.981581798483\n",
      "246 0.982665222102\n",
      "247 0.981581798483\n",
      "248 0.980498374865\n",
      "249 0.910075839653\n",
      "250 0.768147345612\n",
      "251 0.964247020585\n",
      "252 0.977248104009\n",
      "253 0.98374864572\n",
      "254 0.966413867822\n",
      "255 0.967497291441\n",
      "256 0.943661971831\n",
      "257 0.975081256771\n",
      "258 0.96858071506\n",
      "259 0.97616468039\n",
      "260 0.945828819068\n",
      "261 0.951245937161\n",
      "262 0.979414951246\n",
      "263 0.982665222102\n",
      "264 0.969664138678\n",
      "265 0.659804983749\n",
      "266 0.977248104009\n",
      "267 0.972914409534\n",
      "268 0.962080173348\n",
      "269 0.979414951246\n",
      "270 0.949079089924\n",
      "271 0.982665222102\n",
      "272 0.986998916576\n",
      "273 0.949079089924\n",
      "274 0.988082340195\n",
      "275 0.96858071506\n",
      "276 0.98374864572\n",
      "277 0.823401950163\n",
      "278 0.979414951246\n",
      "279 0.984832069339\n",
      "280 0.990249187432\n",
      "281 0.990249187432\n",
      "282 0.981581798483\n",
      "283 0.984832069339\n",
      "284 0.905742145179\n",
      "285 0.985915492958\n",
      "286 0.94474539545\n",
      "287 0.978331527627\n",
      "288 0.966413867822\n",
      "289 0.988082340195\n",
      "290 0.988082340195\n",
      "291 0.969664138678\n",
      "292 0.934994582882\n",
      "293 0.973997833153\n",
      "294 0.989165763814\n",
      "295 0.966413867822\n",
      "296 0.947995666306\n",
      "297 0.990249187432\n",
      "298 0.99241603467\n",
      "299 0.986998916576\n",
      "300 0.970747562297\n",
      "301 0.980498374865\n",
      "302 0.98374864572\n",
      "303 0.966413867822\n",
      "304 0.991332611051\n",
      "305 0.979414951246\n",
      "306 0.993499458288\n",
      "307 0.98374864572\n",
      "308 0.985915492958\n",
      "309 0.99241603467\n",
      "310 0.984832069339\n",
      "311 0.984832069339\n",
      "312 0.988082340195\n",
      "313 0.912242686891\n",
      "314 0.991332611051\n",
      "315 0.991332611051\n",
      "316 0.991332611051\n",
      "317 0.981581798483\n",
      "318 0.994582881907\n",
      "319 0.993499458288\n",
      "320 0.993499458288\n",
      "321 0.99241603467\n",
      "322 0.943661971831\n",
      "323 0.973997833153\n",
      "324 0.99241603467\n",
      "325 0.991332611051\n",
      "326 0.991332611051\n",
      "327 0.919826652221\n",
      "328 0.704225352113\n",
      "329 0.970747562297\n",
      "330 0.911159263272\n",
      "331 0.969664138678\n",
      "332 0.99241603467\n",
      "333 0.972914409534\n",
      "334 0.996749729144\n",
      "335 0.991332611051\n",
      "336 0.990249187432\n",
      "337 0.991332611051\n",
      "338 0.994582881907\n",
      "339 0.994582881907\n",
      "340 0.989165763814\n",
      "341 0.985915492958\n",
      "342 0.989165763814\n",
      "343 0.995666305525\n",
      "344 0.990249187432\n",
      "345 0.990249187432\n",
      "346 0.99241603467\n",
      "347 0.985915492958\n",
      "348 0.996749729144\n",
      "349 0.996749729144\n",
      "350 0.985915492958\n",
      "351 0.988082340195\n",
      "352 0.964247020585\n",
      "353 0.996749729144\n",
      "354 0.99241603467\n",
      "355 0.990249187432\n",
      "356 0.982665222102\n",
      "357 0.991332611051\n",
      "358 0.953412784399\n",
      "359 0.965330444204\n",
      "360 0.991332611051\n",
      "361 0.874322860238\n",
      "362 0.956663055255\n",
      "363 0.996749729144\n",
      "364 0.963163596966\n",
      "365 0.989165763814\n",
      "366 0.986998916576\n",
      "367 0.967497291441\n",
      "368 0.994582881907\n",
      "369 0.995666305525\n",
      "370 0.994582881907\n",
      "371 0.993499458288\n",
      "372 0.994582881907\n",
      "373 0.99241603467\n",
      "374 0.99241603467\n",
      "375 0.994582881907\n",
      "376 0.996749729144\n",
      "377 0.996749729144\n",
      "378 0.956663055255\n",
      "379 0.994582881907\n",
      "380 0.954496208017\n",
      "381 0.998916576381\n",
      "382 0.995666305525\n",
      "383 0.993499458288\n",
      "384 0.990249187432\n",
      "385 0.973997833153\n",
      "386 1.0\n",
      "387 0.993499458288\n",
      "388 0.956663055255\n",
      "389 0.991332611051\n",
      "390 0.998916576381\n",
      "391 1.0\n",
      "392 0.993499458288\n",
      "393 0.998916576381\n",
      "394 0.947995666306\n",
      "395 0.991332611051\n",
      "396 0.955579631636\n",
      "397 0.97616468039\n",
      "398 1.0\n",
      "399 0.989165763814\n",
      "400 0.998916576381\n",
      "401 0.989165763814\n",
      "402 0.998916576381\n",
      "403 0.969664138678\n",
      "404 0.977248104009\n",
      "405 0.990249187432\n",
      "406 0.997833152763\n",
      "407 0.995666305525\n",
      "408 1.0\n",
      "409 1.0\n",
      "410 0.998916576381\n",
      "411 0.997833152763\n",
      "412 1.0\n",
      "413 1.0\n",
      "414 0.988082340195\n",
      "415 0.993499458288\n",
      "416 0.998916576381\n",
      "417 0.990249187432\n",
      "418 1.0\n",
      "419 0.998916576381\n",
      "420 0.993499458288\n",
      "421 0.958829902492\n",
      "422 1.0\n",
      "423 0.994582881907\n",
      "424 0.997833152763\n",
      "425 0.998916576381\n",
      "426 0.998916576381\n",
      "427 0.998916576381\n",
      "428 0.995666305525\n",
      "429 0.997833152763\n",
      "430 0.997833152763\n",
      "431 1.0\n",
      "432 0.996749729144\n",
      "433 1.0\n",
      "434 1.0\n",
      "435 0.998916576381\n",
      "436 0.995666305525\n",
      "437 1.0\n",
      "438 0.993499458288\n",
      "439 1.0\n",
      "440 1.0\n",
      "441 1.0\n",
      "442 1.0\n",
      "443 1.0\n",
      "444 0.998916576381\n",
      "445 1.0\n",
      "446 0.996749729144\n",
      "447 0.996749729144\n",
      "448 1.0\n",
      "449 0.998916576381\n",
      "450 0.993499458288\n",
      "451 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452 1.0\n",
      "453 1.0\n",
      "454 1.0\n",
      "455 0.993499458288\n",
      "456 1.0\n",
      "457 1.0\n",
      "458 0.998916576381\n",
      "459 0.997833152763\n",
      "460 1.0\n",
      "461 1.0\n",
      "462 1.0\n",
      "463 1.0\n",
      "464 1.0\n",
      "465 1.0\n",
      "466 0.998916576381\n",
      "467 1.0\n",
      "468 0.998916576381\n",
      "469 1.0\n",
      "470 0.996749729144\n",
      "471 1.0\n",
      "472 1.0\n",
      "473 1.0\n",
      "474 0.998916576381\n",
      "475 0.997833152763\n",
      "476 1.0\n",
      "477 0.996749729144\n",
      "478 0.995666305525\n",
      "479 1.0\n",
      "480 0.998916576381\n",
      "481 1.0\n",
      "482 0.995666305525\n",
      "483 0.980498374865\n",
      "484 0.998916576381\n",
      "485 1.0\n",
      "486 1.0\n",
      "487 1.0\n",
      "488 1.0\n",
      "489 1.0\n",
      "490 0.997833152763\n",
      "491 0.998916576381\n",
      "492 1.0\n",
      "493 0.997833152763\n",
      "494 0.997833152763\n",
      "495 1.0\n",
      "496 0.990249187432\n",
      "497 1.0\n",
      "498 1.0\n",
      "499 1.0\n",
      "500 1.0\n",
      "501 1.0\n",
      "502 1.0\n",
      "503 0.998916576381\n",
      "504 0.996749729144\n",
      "505 0.995666305525\n",
      "506 0.996749729144\n",
      "507 1.0\n",
      "508 1.0\n",
      "509 1.0\n",
      "510 1.0\n",
      "511 1.0\n",
      "512 1.0\n",
      "513 1.0\n",
      "514 0.996749729144\n",
      "515 1.0\n",
      "516 1.0\n",
      "517 1.0\n",
      "518 1.0\n",
      "519 1.0\n",
      "520 1.0\n",
      "521 0.996749729144\n",
      "522 1.0\n",
      "523 1.0\n",
      "524 1.0\n",
      "525 1.0\n",
      "526 1.0\n",
      "527 1.0\n",
      "528 1.0\n",
      "529 1.0\n",
      "530 1.0\n",
      "531 1.0\n",
      "532 1.0\n",
      "533 0.986998916576\n",
      "534 1.0\n",
      "535 0.994582881907\n",
      "536 1.0\n",
      "537 1.0\n",
      "538 0.965330444204\n",
      "539 1.0\n",
      "540 1.0\n",
      "541 1.0\n",
      "542 1.0\n",
      "543 1.0\n",
      "544 1.0\n",
      "545 1.0\n",
      "546 1.0\n",
      "547 1.0\n",
      "548 0.996749729144\n",
      "549 1.0\n",
      "550 1.0\n",
      "551 1.0\n",
      "552 0.996749729144\n",
      "553 1.0\n",
      "554 0.998916576381\n",
      "555 1.0\n",
      "556 1.0\n",
      "557 1.0\n",
      "558 1.0\n",
      "559 1.0\n",
      "560 1.0\n",
      "561 1.0\n",
      "562 0.998916576381\n",
      "563 1.0\n",
      "564 1.0\n",
      "565 1.0\n",
      "566 1.0\n",
      "567 1.0\n",
      "568 1.0\n",
      "569 1.0\n",
      "570 1.0\n",
      "571 1.0\n",
      "572 1.0\n",
      "573 1.0\n",
      "574 1.0\n",
      "575 1.0\n",
      "576 1.0\n",
      "577 1.0\n",
      "578 1.0\n",
      "579 1.0\n",
      "580 1.0\n",
      "581 1.0\n",
      "582 1.0\n",
      "583 1.0\n",
      "584 0.998916576381\n",
      "585 1.0\n",
      "586 1.0\n",
      "587 1.0\n",
      "588 1.0\n",
      "589 1.0\n",
      "590 1.0\n",
      "591 1.0\n",
      "592 1.0\n",
      "593 1.0\n",
      "594 1.0\n",
      "595 1.0\n",
      "596 1.0\n",
      "597 1.0\n",
      "598 0.998916576381\n",
      "599 1.0\n",
      "600 1.0\n",
      "601 1.0\n",
      "602 1.0\n",
      "603 1.0\n",
      "604 1.0\n",
      "605 1.0\n",
      "606 1.0\n",
      "607 1.0\n",
      "608 1.0\n",
      "609 1.0\n",
      "610 1.0\n",
      "611 1.0\n",
      "612 0.998916576381\n",
      "613 1.0\n",
      "614 1.0\n",
      "615 1.0\n",
      "616 1.0\n",
      "617 1.0\n",
      "618 1.0\n",
      "619 1.0\n",
      "620 1.0\n",
      "621 1.0\n",
      "622 1.0\n",
      "623 1.0\n",
      "624 1.0\n",
      "625 1.0\n",
      "626 1.0\n",
      "627 1.0\n",
      "628 1.0\n",
      "629 1.0\n",
      "630 1.0\n",
      "631 1.0\n",
      "632 1.0\n",
      "633 1.0\n",
      "634 1.0\n",
      "635 1.0\n",
      "636 1.0\n",
      "637 1.0\n",
      "638 1.0\n",
      "639 1.0\n",
      "640 1.0\n",
      "641 1.0\n",
      "642 1.0\n",
      "643 1.0\n",
      "644 1.0\n",
      "645 1.0\n",
      "646 1.0\n",
      "647 1.0\n",
      "648 1.0\n",
      "649 1.0\n",
      "650 1.0\n",
      "651 1.0\n",
      "652 1.0\n",
      "653 1.0\n",
      "654 1.0\n",
      "655 1.0\n",
      "656 1.0\n",
      "657 1.0\n",
      "658 1.0\n",
      "659 1.0\n",
      "660 1.0\n",
      "661 1.0\n",
      "662 1.0\n",
      "663 1.0\n",
      "664 1.0\n",
      "665 1.0\n",
      "666 1.0\n",
      "667 1.0\n",
      "668 1.0\n",
      "669 1.0\n",
      "670 1.0\n",
      "671 1.0\n",
      "672 1.0\n",
      "673 1.0\n",
      "674 1.0\n",
      "675 1.0\n",
      "676 1.0\n",
      "677 1.0\n",
      "678 1.0\n",
      "679 1.0\n",
      "680 1.0\n",
      "681 1.0\n",
      "682 1.0\n",
      "683 1.0\n",
      "684 1.0\n",
      "685 1.0\n",
      "686 1.0\n",
      "687 1.0\n",
      "688 1.0\n",
      "689 1.0\n",
      "690 1.0\n",
      "691 1.0\n",
      "692 1.0\n",
      "693 1.0\n",
      "694 1.0\n",
      "695 1.0\n",
      "696 1.0\n",
      "697 1.0\n",
      "698 1.0\n",
      "699 1.0\n",
      "700 1.0\n",
      "701 1.0\n",
      "702 1.0\n",
      "703 1.0\n",
      "704 1.0\n",
      "705 1.0\n",
      "706 1.0\n",
      "707 1.0\n",
      "708 1.0\n",
      "709 1.0\n",
      "710 1.0\n",
      "711 1.0\n",
      "712 1.0\n",
      "713 1.0\n",
      "714 1.0\n",
      "715 1.0\n",
      "716 1.0\n",
      "717 1.0\n",
      "718 1.0\n",
      "719 1.0\n",
      "720 1.0\n",
      "721 1.0\n",
      "722 1.0\n",
      "723 1.0\n",
      "724 1.0\n",
      "725 1.0\n",
      "726 1.0\n",
      "727 1.0\n",
      "728 1.0\n",
      "729 1.0\n",
      "730 1.0\n",
      "731 1.0\n",
      "732 1.0\n",
      "733 1.0\n",
      "734 1.0\n",
      "735 1.0\n",
      "736 1.0\n",
      "737 1.0\n",
      "738 1.0\n",
      "739 1.0\n",
      "740 1.0\n",
      "741 1.0\n",
      "742 1.0\n",
      "743 1.0\n",
      "744 1.0\n",
      "745 1.0\n",
      "746 1.0\n",
      "747 1.0\n",
      "748 1.0\n",
      "749 1.0\n",
      "750 1.0\n",
      "751 1.0\n",
      "752 1.0\n",
      "753 1.0\n",
      "754 1.0\n",
      "755 1.0\n",
      "756 1.0\n",
      "757 1.0\n",
      "758 1.0\n",
      "759 1.0\n",
      "760 1.0\n",
      "761 1.0\n",
      "762 1.0\n",
      "763 1.0\n",
      "764 1.0\n",
      "765 1.0\n",
      "766 1.0\n",
      "767 1.0\n",
      "768 1.0\n",
      "769 1.0\n",
      "770 1.0\n",
      "771 1.0\n",
      "772 1.0\n",
      "773 1.0\n",
      "774 1.0\n",
      "775 1.0\n",
      "776 1.0\n",
      "777 1.0\n",
      "778 1.0\n",
      "779 1.0\n",
      "780 1.0\n",
      "781 1.0\n",
      "782 1.0\n",
      "783 1.0\n",
      "784 1.0\n",
      "785 1.0\n",
      "786 1.0\n",
      "787 1.0\n",
      "788 1.0\n",
      "789 1.0\n",
      "790 1.0\n",
      "791 1.0\n",
      "792 1.0\n",
      "793 1.0\n",
      "794 1.0\n",
      "795 1.0\n",
      "796 1.0\n",
      "797 1.0\n",
      "798 1.0\n",
      "799 1.0\n",
      "800 1.0\n",
      "801 1.0\n",
      "802 1.0\n",
      "803 1.0\n",
      "804 1.0\n",
      "805 1.0\n",
      "806 1.0\n",
      "807 1.0\n",
      "808 1.0\n",
      "809 1.0\n",
      "810 1.0\n",
      "811 1.0\n",
      "812 1.0\n",
      "813 1.0\n",
      "814 1.0\n",
      "815 1.0\n",
      "816 1.0\n",
      "817 1.0\n",
      "818 1.0\n",
      "819 1.0\n",
      "820 1.0\n",
      "821 1.0\n",
      "822 1.0\n",
      "823 1.0\n",
      "824 1.0\n",
      "825 1.0\n",
      "826 1.0\n",
      "827 1.0\n",
      "828 1.0\n",
      "829 1.0\n",
      "830 1.0\n",
      "831 1.0\n",
      "832 1.0\n",
      "833 1.0\n",
      "834 1.0\n",
      "835 1.0\n",
      "836 1.0\n",
      "837 1.0\n",
      "838 1.0\n",
      "839 1.0\n",
      "840 1.0\n",
      "841 1.0\n",
      "842 1.0\n",
      "843 1.0\n",
      "844 1.0\n",
      "845 1.0\n",
      "846 1.0\n",
      "847 1.0\n",
      "848 1.0\n",
      "849 1.0\n",
      "850 1.0\n",
      "851 1.0\n",
      "852 1.0\n",
      "853 1.0\n",
      "854 1.0\n",
      "855 1.0\n",
      "856 1.0\n",
      "857 1.0\n",
      "858 1.0\n",
      "859 1.0\n",
      "860 1.0\n",
      "861 1.0\n",
      "862 1.0\n",
      "863 1.0\n",
      "864 1.0\n",
      "865 1.0\n",
      "866 1.0\n",
      "867 1.0\n",
      "868 1.0\n",
      "869 1.0\n",
      "870 1.0\n",
      "871 1.0\n",
      "872 1.0\n",
      "873 1.0\n",
      "874 1.0\n",
      "875 1.0\n",
      "876 1.0\n",
      "877 1.0\n",
      "878 1.0\n",
      "879 1.0\n",
      "880 1.0\n",
      "881 1.0\n",
      "882 1.0\n",
      "883 1.0\n",
      "884 1.0\n",
      "885 1.0\n",
      "886 1.0\n",
      "887 1.0\n",
      "888 1.0\n",
      "889 1.0\n",
      "890 1.0\n",
      "891 1.0\n",
      "892 1.0\n",
      "893 1.0\n",
      "894 1.0\n",
      "895 1.0\n",
      "896 1.0\n",
      "897 1.0\n",
      "898 1.0\n",
      "899 1.0\n",
      "900 1.0\n",
      "901 1.0\n",
      "902 1.0\n",
      "903 1.0\n",
      "904 1.0\n",
      "905 1.0\n",
      "906 1.0\n",
      "907 1.0\n",
      "908 1.0\n",
      "909 1.0\n",
      "910 1.0\n",
      "911 1.0\n",
      "912 1.0\n",
      "913 1.0\n",
      "914 1.0\n",
      "915 1.0\n",
      "916 1.0\n",
      "917 1.0\n",
      "918 1.0\n",
      "919 1.0\n",
      "920 1.0\n",
      "921 1.0\n",
      "922 1.0\n",
      "923 1.0\n",
      "924 1.0\n",
      "925 1.0\n",
      "926 1.0\n",
      "927 1.0\n",
      "928 1.0\n",
      "929 1.0\n",
      "930 1.0\n",
      "931 1.0\n",
      "932 1.0\n",
      "933 1.0\n",
      "934 1.0\n",
      "935 1.0\n",
      "936 1.0\n",
      "937 1.0\n",
      "938 1.0\n",
      "939 1.0\n",
      "940 1.0\n",
      "941 1.0\n",
      "942 1.0\n",
      "943 1.0\n",
      "944 1.0\n",
      "945 1.0\n",
      "946 1.0\n",
      "947 1.0\n",
      "948 1.0\n",
      "949 1.0\n",
      "950 1.0\n",
      "951 1.0\n",
      "952 1.0\n",
      "953 1.0\n",
      "954 1.0\n",
      "955 1.0\n",
      "956 1.0\n",
      "957 1.0\n",
      "958 1.0\n",
      "959 1.0\n",
      "960 1.0\n",
      "961 1.0\n",
      "962 1.0\n",
      "963 1.0\n",
      "964 1.0\n",
      "965 1.0\n",
      "966 1.0\n",
      "967 1.0\n",
      "968 1.0\n",
      "969 1.0\n",
      "970 1.0\n",
      "971 1.0\n",
      "972 1.0\n",
      "973 1.0\n",
      "974 1.0\n",
      "975 1.0\n",
      "976 1.0\n",
      "977 1.0\n",
      "978 1.0\n",
      "979 1.0\n",
      "980 1.0\n",
      "981 1.0\n",
      "982 1.0\n",
      "983 1.0\n",
      "984 1.0\n",
      "985 1.0\n",
      "986 1.0\n",
      "987 1.0\n",
      "988 1.0\n",
      "989 1.0\n",
      "990 1.0\n",
      "991 1.0\n",
      "992 1.0\n",
      "993 1.0\n",
      "994 1.0\n",
      "995 1.0\n",
      "996 1.0\n",
      "997 1.0\n",
      "998 1.0\n",
      "999 1.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        p = np.random.permutation(range(len(trX)))\n",
    "        trX, trY = trX[p], trY[p]\n",
    "        \n",
    "        for start in range(0, len(trX), BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
    "        \n",
    "        accuracy = np.mean(np.argmax(trY, axis=1) == \n",
    "                             sess.run(predict_op, feed_dict={X: trX, Y: trY}))\n",
    "        print(epoch, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = np.arange(1, 101)\n",
    "teX = np.transpose(binary_encode(numbers, NUM_DIGITS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz'\n",
      " 'buzz' '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' '34' 'buzz'\n",
      " 'fizz' '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47'\n",
      " 'fizz' '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'fizz' '67' '68' 'fizz' 'buzz'\n",
      " '71' 'fizz' '73' '74' 'fizzbuzz' '76' '77' 'fizz' '79' 'buzz' 'fizz' '82'\n",
      " '83' 'fizz' 'buzz' '86' 'fizz' '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' 'buzz']\n"
     ]
    }
   ],
   "source": [
    "teY = sess.run(predict_op, feed_dict={X: teX})\n",
    "output = np.vectorize(fizz_buzz)(numbers, teY)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the improved network made it! It gave us a correct result of the fizz buzz problem.\n",
    "\n",
    "結果，修改過的網路真的學會如何分辨Fizz Buzz了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = [fizz_buzz(i, fizz_buzz_encode(i).argmax()) for i in numbers]\n",
    "\n",
    "for i, (predicted, actual) in enumerate(zip(output, actuals)):\n",
    "    if predicted != actual:\n",
    "        print(\"{0} {1} {2}\".format(i+1, predicted, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to machine learning again! I believe I will get a job next time!\n",
    "\n",
    "可喜可賀！再次謝謝機器學習！相信下次一定能拿到工作的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
